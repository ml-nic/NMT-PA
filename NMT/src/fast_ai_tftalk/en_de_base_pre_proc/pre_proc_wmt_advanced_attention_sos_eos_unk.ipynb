{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NMT Model\n",
    "\n",
    "This example was taken from the wonderful Cutting Edge Deep Learning for Coders course as taught by Jeremy Howard http://course.fast.ai/part2.html The course is now live and I encourage you to check it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "#import sutils; importlib.reload(sutils)\n",
    "from sutils import *\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import keras.backend as K\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras import initializers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, TensorBoard\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "\n",
    "from recurrentshop import *\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq,SimpleSeq2Seq, Seq2Seq\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we will use **gensim** and **word2vec** to get our embeddings for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = True\n",
    "EOS = True\n",
    "UNK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../neural_translation_en_de_attention/'\n",
    "dpath = '../neural_translation_en_de_attention/translate/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(dpath):\n",
    "    os.makedirs(dpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Corpus\n",
    "\n",
    "we will make a limited corpus of English Questions and their partners in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050001\n",
      "However, what guarantee would we have that they would not use monopoly power to price these books above the range of ordinary citizens?\tWelche Garantie hätten wir jedoch, dass das Unternehmen seine Monopolstellung nicht dazu ausnutzen würde, die Preise dieser Bücher über dem festzulegen, was normale Bürger sich leisten können?\n"
     ]
    }
   ],
   "source": [
    "# Split train file in two seperat for each language one:\n",
    "\n",
    "lines = open('../DE_EN_(wmt16_google_nmt)_train.txt', encoding='UTF-8').read().split('\\n')\n",
    "print(len(lines))\n",
    "print(lines[0])\n",
    "lines_de = []\n",
    "lines_en = []\n",
    "for line in lines:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    lines_en.append(input_text)\n",
    "    lines_de.append(target_text)\n",
    "assert len(lines_de) == len(lines_en)    \n",
    "\n",
    "with(open('../DE_EN_(wmt16_google_nmt)_train_german_only.txt', 'w', encoding='utf8')) as file:\n",
    "    for idx in range(len(lines_de)):\n",
    "        file.write(lines_de[idx])\n",
    "        if idx != (len(lines_de) - 1):\n",
    "            file.write(\"\\n\")\n",
    "with(open('../DE_EN_(wmt16_google_nmt)_train_english_only.txt', 'w', encoding='utf8')) as file:\n",
    "    for idx in range(len(lines_en)):\n",
    "        file.write(lines_en[idx])\n",
    "        if idx != (len(lines_en) - 1):\n",
    "            file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../\" + 'DE_EN_(wmt16_google_nmt)_train_'\n",
    "en_fname = fname + 'english_only.txt'\n",
    "fr_fname = fname + 'german_only.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates the Regex for filtering just for questions\n",
    "#re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "#re_fq = re.compile('^([^?.!]+\\?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this runs our regex search on the full corpus and filters it down\n",
    "#lines = ((re_eq.search(eq), re_fq.search(fq)) \n",
    "#         for eq, fq in zip(open(en_fname, encoding='utf8'), open(fr_fname, encoding='utf8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('However, what guarantee would we have that they would not use monopoly power to price these books above the range of ordinary citizens?',\n",
       "  'Welche Garantie hätten wir jedoch, dass das Unternehmen seine Monopolstellung nicht dazu ausnutzen würde, die Preise dieser Bücher über dem festzulegen, was normale Bürger sich leisten können?'),\n",
       " ('The debate is closed.', 'Die Aussprache ist geschlossen.'),\n",
       " ('That is why there are so few amendments and, furthermore, they promote the message which we must send at the beginning of the process, above all to the other wing of the budgetary authority.',\n",
       "  'So kommt es, dass es wenig Änderungsanträge gibt, und damit wird die Botschaft unterstützt, die wir zu Beginn des Verfahrens vor allem an den anderen Arm der Haushaltsbehörde aussenden müssen.'),\n",
       " ('If necessary, my colleague Evelyne Gebhardt from the Committee on Legal Affairs will represent me.',\n",
       "  'In diesem Fall würde mich die Kollegin Evelyne Gebhardt aus dem Rechtsausschuß vertreten.'),\n",
       " ('Under the programme, one academic partner and one industrial partner from India and Germany will develop a particular air bearing by the use of mouldable techniques.',\n",
       "  'In diesem Programm werden ein akademischer und ein industrieller Partner aus Indien und aus Deutschland besondere Luftlager herstellen mit der Abform-Technologie.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [(en, de) for en, de in zip(open(en_fname, encoding='utf8').read().split('\\n'), open(fr_fname, encoding='utf8').read().split('\\n'))]\n",
    "questions = lines\n",
    "questions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to put them all in a list so that we can easily access them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = [(e.group(), f.group()) for e,f in lines if e and f]\n",
    "#len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets save this so we can come back to it in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(questions, dpath+'questionswmt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "loading and unwrapping the raw English/French questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = load(dpath+'questionswmt.pkl')\n",
    "en_qs, fr_qs = zip(*questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the questions into tokens so that we can make sequences for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_mult_space = re.compile(r\"  *\")\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\")\n",
    "re_apos = re.compile(r\"(\\w)'s\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_toks(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welche',\n",
       "  'garantie',\n",
       "  'hätten',\n",
       "  'wir',\n",
       "  'jedoch',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'das',\n",
       "  'unternehmen',\n",
       "  'seine',\n",
       "  'monopolstellung',\n",
       "  'nicht',\n",
       "  'dazu',\n",
       "  'ausnutzen',\n",
       "  'würde',\n",
       "  ',',\n",
       "  'die',\n",
       "  'preise',\n",
       "  'dieser',\n",
       "  'bücher',\n",
       "  'über',\n",
       "  'dem',\n",
       "  'festzulegen',\n",
       "  ',',\n",
       "  'was',\n",
       "  'normale',\n",
       "  'bürger',\n",
       "  'sich',\n",
       "  'leisten',\n",
       "  'können',\n",
       "  '?'],\n",
       " ['die', 'aussprache', 'ist', 'geschlossen', '.'],\n",
       " ['so',\n",
       "  'kommt',\n",
       "  'es',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'es',\n",
       "  'wenig',\n",
       "  'änderungsanträge',\n",
       "  'gibt',\n",
       "  ',',\n",
       "  'und',\n",
       "  'damit',\n",
       "  'wird',\n",
       "  'die',\n",
       "  'botschaft',\n",
       "  'unterstützt',\n",
       "  ',',\n",
       "  'die',\n",
       "  'wir',\n",
       "  'zu',\n",
       "  'beginn',\n",
       "  'des',\n",
       "  'verfahrens',\n",
       "  'vor',\n",
       "  'allem',\n",
       "  'an',\n",
       "  'den',\n",
       "  'anderen',\n",
       "  'arm',\n",
       "  'der',\n",
       "  'haushaltsbehörde',\n",
       "  'aussenden',\n",
       "  'müssen',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'diesem',\n",
       "  'fall',\n",
       "  'würde',\n",
       "  'mich',\n",
       "  'die',\n",
       "  'kollegin',\n",
       "  'evelyne',\n",
       "  'gebhardt',\n",
       "  'aus',\n",
       "  'dem',\n",
       "  'rechtsausschuß',\n",
       "  'vertreten',\n",
       "  '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_qtoks = list(map(simple_toks, fr_qs)); fr_qtoks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['however',\n",
       "  ',',\n",
       "  'what',\n",
       "  'guarantee',\n",
       "  'would',\n",
       "  'we',\n",
       "  'have',\n",
       "  'that',\n",
       "  'they',\n",
       "  'would',\n",
       "  'not',\n",
       "  'use',\n",
       "  'monopoly',\n",
       "  'power',\n",
       "  'to',\n",
       "  'price',\n",
       "  'these',\n",
       "  'books',\n",
       "  'above',\n",
       "  'the',\n",
       "  'range',\n",
       "  'of',\n",
       "  'ordinary',\n",
       "  'citizens',\n",
       "  '?'],\n",
       " ['the', 'debate', 'is', 'closed', '.'],\n",
       " ['that',\n",
       "  'is',\n",
       "  'why',\n",
       "  'there',\n",
       "  'are',\n",
       "  'so',\n",
       "  'few',\n",
       "  'amendments',\n",
       "  'and',\n",
       "  ',',\n",
       "  'furthermore',\n",
       "  ',',\n",
       "  'they',\n",
       "  'promote',\n",
       "  'the',\n",
       "  'message',\n",
       "  'which',\n",
       "  'we',\n",
       "  'must',\n",
       "  'send',\n",
       "  'at',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'the',\n",
       "  'process',\n",
       "  ',',\n",
       "  'above',\n",
       "  'all',\n",
       "  'to',\n",
       "  'the',\n",
       "  'other',\n",
       "  'wing',\n",
       "  'of',\n",
       "  'the',\n",
       "  'budgetary',\n",
       "  'authority',\n",
       "  '.'],\n",
       " ['if',\n",
       "  'necessary',\n",
       "  ',',\n",
       "  'my',\n",
       "  'colleague',\n",
       "  'evelyne',\n",
       "  'gebhardt',\n",
       "  'from',\n",
       "  'the',\n",
       "  'committee',\n",
       "  'on',\n",
       "  'legal',\n",
       "  'affairs',\n",
       "  'will',\n",
       "  'represent',\n",
       "  'me',\n",
       "  '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_qtoks = list(map(simple_toks, en_qs)); en_qtoks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert tokens to ids so that we can creat lookup tables   \n",
    "\n",
    "we also insert the \"PAD\" token in here\n",
    "\n",
    "this function returns\n",
    "ids - for words\n",
    "vocab -  \n",
    "w2id - is for looking up the \n",
    "voc_cnt - the vocab count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks2ids(sents, are_source_sentences=False):\n",
    "    voc_cnt = collections.Counter(t for sent in sents for t in sent)\n",
    "    vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n",
    "    \n",
    "    pad_id = 0\n",
    "    sos_id = 1\n",
    "    eos_id = 1\n",
    "    unk_id = 1\n",
    "    \n",
    "    vocab.insert(pad_id, \"<PAD>\")\n",
    "    if SOS:\n",
    "        vocab.insert(sos_id, \"<SOS>\")\n",
    "        eos_id +=1\n",
    "        unk_id +=1\n",
    "    if EOS:\n",
    "        vocab.insert(eos_id, \"<EOS>\")\n",
    "        unk_id += 1\n",
    "    if are_source_sentences and UNK:\n",
    "        vocab.insert(unk_id, \"<UNK>\")\n",
    "    \n",
    "    vocab = vocab[0:40003]\n",
    "    w2id = {w:i for i,w in enumerate(vocab) if i < 40003}\n",
    "    ids = []\n",
    "    for sent in sents:\n",
    "        sent_ids = []\n",
    "        if SOS == True:\n",
    "            sent_ids.append(w2id[\"<SOS>\"])\n",
    "        for t in sent:\n",
    "            try:\n",
    "                sent_ids.append(w2id[t])\n",
    "            except KeyError:\n",
    "                if are_source_sentences and UNK:\n",
    "                    sent_ids.append(w2id[\"<UNK>\"])\n",
    "        if EOS:\n",
    "            sent_ids.append(w2id[\"<EOS>\"])\n",
    "        ids.append(sent_ids)\n",
    "    print(len(ids),len(vocab),len(w2id),len(voc_cnt))\n",
    "    return ids, vocab, w2id, voc_cnt, pad_id, sos_id, eos_id, unk_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050001 40003 40003 677741\n",
      "0 1 2 3\n",
      "1050001 40003 40003 343772\n",
      "0 1 2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40003, 40003)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_ids, fr_vocab, fr_w2id, fr_counts, pad_id, sos_id, eos_id, unk_id = toks2ids(fr_qtoks)\n",
    "print(pad_id, sos_id, eos_id, unk_id )\n",
    "en_ids, en_vocab, en_w2id, en_counts, pad_id, sos_id, eos_id, unk_id = toks2ids(en_qtoks, are_source_sentences=True)\n",
    "print(pad_id, sos_id, eos_id, unk_id )\n",
    "len(en_vocab), len(fr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences converted to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 243, 12, 1059, 6, 2]\n",
      "['the', 'debate', 'is', 'closed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(en_ids[1])\n",
    "print(en_qtoks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The look up tables / dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_w2id['do']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "here we are going to make look up tables for words to embeddings\n",
    "\n",
    "The GloVE embeddings used here are 400k words with 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_load_glove(loc):\n",
    "    return (load_array(loc+'.txt'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb'), encoding='latin1'),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb'), encoding='latin1'))\n",
    "def load_glove(loc):\n",
    "    en_wv_word = []\n",
    "    en_wv_idx = {}\n",
    "    en_vecs = []\n",
    "    loc = loc + '.txt'\n",
    "    with(open(loc, encoding='latin1')) as file:\n",
    "        lines = file.readlines()\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        splitted_lines = line.split(' ')\n",
    "        word = splitted_lines[0]\n",
    "        en_wv_word.append(word)\n",
    "        en_wv_idx[word] = index\n",
    "        index += 1\n",
    "        en_vecs.append(splitted_lines[1:len(splitted_lines)])\n",
    "    return (np.asarray(en_vecs), en_wv_word, en_wv_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs, en_wv_word, en_wv_idx = load_glove('../embeddings/glove/6B.200d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_w2v = {w: en_vecs[en_wv_idx[w]] for w in en_wv_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_en_vec, dim_en_vec = en_vecs.shape\n",
    "dim_fr_vec = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_en_vec 200\n",
      "n_en_vec 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_en_vec\", dim_en_vec)\n",
    "print(\"n_en_vec\", n_en_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fr_wik = pickle.load(open('/data/TensorFlowTalks/embeddings/french/polyglot-fr.pkl', 'rb'), \n",
    "#                     encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The French embeddings were trained by Jean-Philippe Fauconnier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Word vectors: http://fauconnier.github.io/index.html#wordembeddingmodels\n",
    "- frWac: http://wacky.sslmit.unibo.it/doku.php?id=corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path='../embeddings/german/word_emb_de.bin'\n",
    "\n",
    "fr_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "fr_voc = fr_model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(w2v, targ_vocab, dim_vec):\n",
    "    vocab_size = 40003\n",
    "    emb = np.zeros((vocab_size, dim_vec))\n",
    "\n",
    "    for i, word in enumerate(targ_vocab):\n",
    "        if i >= vocab_size:\n",
    "            break\n",
    "        try:\n",
    "            emb[i] = w2v[word]\n",
    "        except KeyError:\n",
    "            # If we can't find the word, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(dim_vec,))\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40003, 200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embs = create_emb(en_w2v, en_vocab, dim_en_vec); en_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40003, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_embs = create_emb(fr_model, fr_vocab, dim_fr_vec); fr_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lengths = collections.Counter(len(s) for s in en_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_padded = pad_sequences(en_ids, maxlen, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_padded = pad_sequences(fr_ids, maxlen, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 0 1\n"
     ]
    }
   ],
   "source": [
    "print(eos_id,unk_id,pad_id,sos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EOS:\n",
    "    for sent in en_padded:\n",
    "        if sent[maxlen-1] != pad_id and sent[maxlen-1] != eos_id:\n",
    "            sent[maxlen-1] = eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1   137    24    39   424  1290    31  7973   673     8   167   207\n",
      "    15   272     6     3  6960   457  1068  1071  9712    17  8874     6\n",
      "    10   553     5    38     7    37  1071   272     6     3  6960   457\n",
      "  1068  9712    17  8874   634   141   549    87    22  5032     5 13117\n",
      "  1669     5     8    72    34 24871    72 24755  3705     6    18  3446\n",
      "  1326  2157     5  3446  1703    60     5  3446   694  3027   766     5\n",
      "   320  2869  4574     5   320   533   463   448   297     5   737  2805\n",
      "     9  1256    32    93  1109    43 16041     6  4176     5   220   680\n",
      "  1109    43  6862     2]\n",
      "[   1  134    5   76  766   51   18   26   13   56   51   27  103 5878  329\n",
      "    9  434   61 2108  484    4  423    7 3271  303   77    2    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(en_padded[4915])\n",
    "print(en_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EOS:\n",
    "    for sent in fr_padded:\n",
    "        if sent[maxlen-1] != pad_id and sent[maxlen-1] != eos_id:\n",
    "            sent[maxlen-1] = eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1050001, 100), (1050001, 100), (40003, 200))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_padded.shape, fr_padded.shape, en_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(en_ids)*0.9)\n",
    "idxs = np.random.permutation(len(en_ids))\n",
    "fr_train, fr_test = fr_padded[idxs][:n], fr_padded[idxs][n:]\n",
    "en_train, en_test = en_padded[idxs][:n], en_padded[idxs][n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_test_words = []\n",
    "with(open('../pre_proc_wmt_advanced_attention_sos_eos_unk_validation_data.txt', 'w', encoding='utf8')) as file:\n",
    "    for idx in idxs[n:]:\n",
    "        file.write(questions[idx][1])\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    98,  5474,  5040,    98,   199,  1652,    10,   289,\n",
       "           8,     4,   579,     7, 27618, 28531,     6,     2,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_train = en_train[0:100000]\n",
    "#fr_train = fr_train[0:100000]\n",
    "\n",
    "#en_test = en_test[0:10000]\n",
    "#fr_test = fr_test[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> 1\n",
      "diese 48\n",
      "erkenntnis 6048\n",
      "beeinflusste 30363\n",
      "seinen 251\n",
      "weiteren 490\n",
      "und 7\n",
      "die 5\n",
      "geschichte 519\n",
      "von 10\n",
      "ebm 38355\n",
      "papst 8058\n",
      ". 4\n",
      "<EOS> 2\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n",
      "<PAD> 0\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict((i, word) for word, i in fr_w2id.items())\n",
    "for a in fr_train[0]:\n",
    "    print(reverse_word_index[a], a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to save  \n",
    "look_ups = {'en_w2id':en_w2id,'fr_vocab':fr_vocab,'en_vocab':en_vocab, 'en_embs':en_embs,'fr_embs':fr_embs}\n",
    "dump(look_ups, dpath+'look_upswmtsmall_sos_eos_unk_att.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'fr_train':fr_train,'en_train':en_train,'fr_test':fr_test,'en_test':en_test,}\n",
    "dump(data, dpath+'nmt_datawmtsmall_sos_eos_unk_att.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0266aff821744178bf198185fbbd5cf1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e7384b4c75d437bb1fe8a9693658cb9": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e8fac4707f042aa903fb0d99a10db34": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "10ab8aa7a96e46bfb356fb50841de35b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "1a324d73121342268fb3bdab4e0ddc85": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "26e3d517d1ad4ef4bb1a8667f380e84f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "38965a9c75124e138d24ea4b3f1aaf51": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "39d8f35bd1bf412db5e388165d2e681d": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "4e1524bdbc094d2aaf762062679cad12": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "54e930da987a4d388a10f95ef9d0d29f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "60a3f36c191a48a68b0b5d7043d7ee2c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "7beff7f32b174d6a84b40a07a84298a6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "852f44c6c8ac41a8bf8b52261d56025a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "85514cbcb43b4a53abdd0fb2d1fe9edc": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a27aec095a34b64b559873253a8ceb6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a63b516aeb9441abdff56dd973c2dcb": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "9921ef0dc7714826b9a4dd210002cadd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a1e4974f09e7444d9e724242cd48193c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a53dd4f9998848ca9ac1b303a729dc9a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "abdda4152496465d8e592203c90dc443": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "c48ca8a0c97a4eb1bdd6ace69e114ffd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ce4ee282c7f643649dee064b54a53748": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "def4852d68a14e1ea7576b27b67b6f17": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e04977fa4cf64be59e9e55bcb898f3d7": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e08e2127395a4003a14814472a0af7f4": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ec407f689bac48bab95ac233b45a8976": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eca29ad2f8734cafab240977b6d5b4f1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "edc814ed0e054bafab4417b5fef593e1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eeb590879c164660ae58bf4f4bd18d25": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "fb22a8a5bcda4a97a91076cf88190fd1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ff9739632a7640d8984e8803859c050b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
