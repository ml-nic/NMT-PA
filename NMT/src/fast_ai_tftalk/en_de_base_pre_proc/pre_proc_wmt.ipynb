{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NMT Model\n",
    "\n",
    "This example was taken from the wonderful Cutting Edge Deep Learning for Coders course as taught by Jeremy Howard http://course.fast.ai/part2.html The course is now live and I encourage you to check it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "#import sutils; importlib.reload(sutils)\n",
    "from sutils import *\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import keras.backend as K\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras import initializers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, TensorBoard\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "\n",
    "from recurrentshop import *\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq,SimpleSeq2Seq, Seq2Seq\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we will use **gensim** and **word2vec** to get our embeddings for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../neural_translation_en_de/'\n",
    "dpath = '../neural_translation_en_de/translate/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(dpath):\n",
    "    os.makedirs(dpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Corpus\n",
    "\n",
    "we will make a limited corpus of English Questions and their partners in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050001\n",
      "However, what guarantee would we have that they would not use monopoly power to price these books above the range of ordinary citizens?\tWelche Garantie hätten wir jedoch, dass das Unternehmen seine Monopolstellung nicht dazu ausnutzen würde, die Preise dieser Bücher über dem festzulegen, was normale Bürger sich leisten können?\n"
     ]
    }
   ],
   "source": [
    "# Split train file in two seperat for each language one:\n",
    "\n",
    "lines = open('../DE_EN_(wmt16_google_nmt)_train.txt', encoding='UTF-8').read().split('\\n')\n",
    "print(len(lines))\n",
    "print(lines[0])\n",
    "lines_de = []\n",
    "lines_en = []\n",
    "for line in lines:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    lines_en.append(input_text)\n",
    "    lines_de.append(target_text)\n",
    "assert len(lines_de) == len(lines_en)    \n",
    "\n",
    "with(open('../DE_EN_(wmt16_google_nmt)_train_german_only.txt', 'w', encoding='utf8')) as file:\n",
    "    for idx in range(len(lines_de)):\n",
    "        file.write(lines_de[idx])\n",
    "        if idx != (len(lines_de) - 1):\n",
    "            file.write(\"\\n\")\n",
    "with(open('../DE_EN_(wmt16_google_nmt)_train_english_only.txt', 'w', encoding='utf8')) as file:\n",
    "    for idx in range(len(lines_en)):\n",
    "        file.write(lines_en[idx])\n",
    "        if idx != (len(lines_en) - 1):\n",
    "            file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../\" + 'DE_EN_(wmt16_google_nmt)_train_'\n",
    "en_fname = fname + 'english_only.txt'\n",
    "fr_fname = fname + 'german_only.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates the Regex for filtering just for questions\n",
    "#re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "#re_fq = re.compile('^([^?.!]+\\?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this runs our regex search on the full corpus and filters it down\n",
    "#lines = ((re_eq.search(eq), re_fq.search(fq)) \n",
    "#         for eq, fq in zip(open(en_fname, encoding='utf8'), open(fr_fname, encoding='utf8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('However, what guarantee would we have that they would not use monopoly power to price these books above the range of ordinary citizens?',\n",
       "  'Welche Garantie hätten wir jedoch, dass das Unternehmen seine Monopolstellung nicht dazu ausnutzen würde, die Preise dieser Bücher über dem festzulegen, was normale Bürger sich leisten können?'),\n",
       " ('The debate is closed.', 'Die Aussprache ist geschlossen.'),\n",
       " ('That is why there are so few amendments and, furthermore, they promote the message which we must send at the beginning of the process, above all to the other wing of the budgetary authority.',\n",
       "  'So kommt es, dass es wenig Änderungsanträge gibt, und damit wird die Botschaft unterstützt, die wir zu Beginn des Verfahrens vor allem an den anderen Arm der Haushaltsbehörde aussenden müssen.'),\n",
       " ('If necessary, my colleague Evelyne Gebhardt from the Committee on Legal Affairs will represent me.',\n",
       "  'In diesem Fall würde mich die Kollegin Evelyne Gebhardt aus dem Rechtsausschuß vertreten.'),\n",
       " ('Under the programme, one academic partner and one industrial partner from India and Germany will develop a particular air bearing by the use of mouldable techniques.',\n",
       "  'In diesem Programm werden ein akademischer und ein industrieller Partner aus Indien und aus Deutschland besondere Luftlager herstellen mit der Abform-Technologie.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [(en, de) for en, de in zip(open(en_fname, encoding='utf8').read().split('\\n'), open(fr_fname, encoding='utf8').read().split('\\n'))]\n",
    "questions = lines\n",
    "questions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to put them all in a list so that we can easily access them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = [(e.group(), f.group()) for e,f in lines if e and f]\n",
    "#len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets save this so we can come back to it in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(questions, dpath+'questionswmt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "loading and unwrapping the raw English/French questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = load(dpath+'questionswmt.pkl')\n",
    "en_qs, fr_qs = zip(*questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the questions into tokens so that we can make sequences for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_mult_space = re.compile(r\"  *\")\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\")\n",
    "re_apos = re.compile(r\"(\\w)'s\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_toks(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welche',\n",
       "  'garantie',\n",
       "  'hätten',\n",
       "  'wir',\n",
       "  'jedoch',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'das',\n",
       "  'unternehmen',\n",
       "  'seine',\n",
       "  'monopolstellung',\n",
       "  'nicht',\n",
       "  'dazu',\n",
       "  'ausnutzen',\n",
       "  'würde',\n",
       "  ',',\n",
       "  'die',\n",
       "  'preise',\n",
       "  'dieser',\n",
       "  'bücher',\n",
       "  'über',\n",
       "  'dem',\n",
       "  'festzulegen',\n",
       "  ',',\n",
       "  'was',\n",
       "  'normale',\n",
       "  'bürger',\n",
       "  'sich',\n",
       "  'leisten',\n",
       "  'können',\n",
       "  '?'],\n",
       " ['die', 'aussprache', 'ist', 'geschlossen', '.'],\n",
       " ['so',\n",
       "  'kommt',\n",
       "  'es',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'es',\n",
       "  'wenig',\n",
       "  'änderungsanträge',\n",
       "  'gibt',\n",
       "  ',',\n",
       "  'und',\n",
       "  'damit',\n",
       "  'wird',\n",
       "  'die',\n",
       "  'botschaft',\n",
       "  'unterstützt',\n",
       "  ',',\n",
       "  'die',\n",
       "  'wir',\n",
       "  'zu',\n",
       "  'beginn',\n",
       "  'des',\n",
       "  'verfahrens',\n",
       "  'vor',\n",
       "  'allem',\n",
       "  'an',\n",
       "  'den',\n",
       "  'anderen',\n",
       "  'arm',\n",
       "  'der',\n",
       "  'haushaltsbehörde',\n",
       "  'aussenden',\n",
       "  'müssen',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'diesem',\n",
       "  'fall',\n",
       "  'würde',\n",
       "  'mich',\n",
       "  'die',\n",
       "  'kollegin',\n",
       "  'evelyne',\n",
       "  'gebhardt',\n",
       "  'aus',\n",
       "  'dem',\n",
       "  'rechtsausschuß',\n",
       "  'vertreten',\n",
       "  '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_qtoks = list(map(simple_toks, fr_qs)); fr_qtoks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['however',\n",
       "  ',',\n",
       "  'what',\n",
       "  'guarantee',\n",
       "  'would',\n",
       "  'we',\n",
       "  'have',\n",
       "  'that',\n",
       "  'they',\n",
       "  'would',\n",
       "  'not',\n",
       "  'use',\n",
       "  'monopoly',\n",
       "  'power',\n",
       "  'to',\n",
       "  'price',\n",
       "  'these',\n",
       "  'books',\n",
       "  'above',\n",
       "  'the',\n",
       "  'range',\n",
       "  'of',\n",
       "  'ordinary',\n",
       "  'citizens',\n",
       "  '?'],\n",
       " ['the', 'debate', 'is', 'closed', '.'],\n",
       " ['that',\n",
       "  'is',\n",
       "  'why',\n",
       "  'there',\n",
       "  'are',\n",
       "  'so',\n",
       "  'few',\n",
       "  'amendments',\n",
       "  'and',\n",
       "  ',',\n",
       "  'furthermore',\n",
       "  ',',\n",
       "  'they',\n",
       "  'promote',\n",
       "  'the',\n",
       "  'message',\n",
       "  'which',\n",
       "  'we',\n",
       "  'must',\n",
       "  'send',\n",
       "  'at',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'the',\n",
       "  'process',\n",
       "  ',',\n",
       "  'above',\n",
       "  'all',\n",
       "  'to',\n",
       "  'the',\n",
       "  'other',\n",
       "  'wing',\n",
       "  'of',\n",
       "  'the',\n",
       "  'budgetary',\n",
       "  'authority',\n",
       "  '.'],\n",
       " ['if',\n",
       "  'necessary',\n",
       "  ',',\n",
       "  'my',\n",
       "  'colleague',\n",
       "  'evelyne',\n",
       "  'gebhardt',\n",
       "  'from',\n",
       "  'the',\n",
       "  'committee',\n",
       "  'on',\n",
       "  'legal',\n",
       "  'affairs',\n",
       "  'will',\n",
       "  'represent',\n",
       "  'me',\n",
       "  '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_qtoks = list(map(simple_toks, en_qs)); en_qtoks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert tokens to ids so that we can creat lookup tables   \n",
    "\n",
    "we also insert the \"PAD\" token in here\n",
    "\n",
    "this function returns\n",
    "ids - for words\n",
    "vocab -  \n",
    "w2id - is for looking up the \n",
    "voc_cnt - the vocab count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks2ids(sents):\n",
    "    voc_cnt = collections.Counter(t for sent in sents for t in sent)\n",
    "    vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n",
    "    vocab.insert(0, \"<PAD>\")\n",
    "    vocab.insert(1, \"<UNK>\")\n",
    "    vocab = vocab[0:40002]\n",
    "    w2id = {w:i for i,w in enumerate(vocab) if i < 40002}\n",
    "    ids = []\n",
    "    for sent in sents:\n",
    "        sent_ids = []\n",
    "        for t in sent:\n",
    "            try:\n",
    "                sent_ids.append(w2id[t])\n",
    "            except KeyError:\n",
    "                sent_ids.append(w2id[\"<UNK>\"])\n",
    "        ids.append(sent_ids)\n",
    "    #ids = [[w2id[t] for t in sent] for sent in sents]\n",
    "    print(len(ids),len(vocab),len(w2id),len(voc_cnt))\n",
    "    return ids, vocab, w2id, voc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050001 40002 40002 677741\n",
      "1050001 40002 40002 343772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40002, 40002)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_ids, fr_vocab, fr_w2id, fr_counts = toks2ids(fr_qtoks)\n",
    "en_ids, en_vocab, en_w2id, en_counts = toks2ids(en_qtoks)\n",
    "len(en_vocab), len(fr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences converted to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 241, 10, 1057, 4]\n",
      "['the', 'debate', 'is', 'closed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(en_ids[1])\n",
    "print(en_qtoks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The look up tables / dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_w2id['do']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "here we are going to make look up tables for words to embeddings\n",
    "\n",
    "The GloVE embeddings used here are 400k words with 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_load_glove(loc):\n",
    "    return (load_array(loc+'.txt'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb'), encoding='latin1'),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb'), encoding='latin1'))\n",
    "def load_glove(loc):\n",
    "    en_wv_word = []\n",
    "    en_wv_idx = {}\n",
    "    en_vecs = []\n",
    "    loc = loc + '.txt'\n",
    "    with(open(loc, encoding='latin1')) as file:\n",
    "        lines = file.readlines()\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        splitted_lines = line.split(' ')\n",
    "        word = splitted_lines[0]\n",
    "        en_wv_word.append(word)\n",
    "        en_wv_idx[word] = index\n",
    "        index += 1\n",
    "        en_vecs.append(splitted_lines[1:len(splitted_lines)])\n",
    "    return (np.asarray(en_vecs), en_wv_word, en_wv_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs, en_wv_word, en_wv_idx = load_glove('../embeddings/glove/6B.200d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_w2v = {w: en_vecs[en_wv_idx[w]] for w in en_wv_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_en_vec, dim_en_vec = en_vecs.shape\n",
    "dim_fr_vec = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_en_vec 200\n",
      "n_en_vec 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_en_vec\", dim_en_vec)\n",
    "print(\"n_en_vec\", n_en_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fr_wik = pickle.load(open('/data/TensorFlowTalks/embeddings/french/polyglot-fr.pkl', 'rb'), \n",
    "#                     encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The French embeddings were trained by Jean-Philippe Fauconnier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Word vectors: http://fauconnier.github.io/index.html#wordembeddingmodels\n",
    "- frWac: http://wacky.sslmit.unibo.it/doku.php?id=corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path='../embeddings/german/word_emb_de.bin'\n",
    "\n",
    "fr_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "fr_voc = fr_model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(w2v, targ_vocab, dim_vec):\n",
    "    vocab_size = 40002\n",
    "    emb = np.zeros((vocab_size, dim_vec))\n",
    "\n",
    "    for i, word in enumerate(targ_vocab):\n",
    "        if i >= vocab_size:\n",
    "            break\n",
    "        try:\n",
    "            emb[i] = w2v[word]\n",
    "        except KeyError:\n",
    "            # If we can't find the word, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(dim_vec,))\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40002, 200)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embs = create_emb(en_w2v, en_vocab, dim_en_vec); en_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40002, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_embs = create_emb(fr_model, fr_vocab, dim_fr_vec); fr_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lengths = collections.Counter(len(s) for s in en_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_padded = pad_sequences(en_ids, maxlen, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_padded = pad_sequences(fr_ids, maxlen, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1050001, 100), (1050001, 100), (40002, 200))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_padded.shape, fr_padded.shape, en_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(en_ids)*0.9)\n",
    "idxs = np.random.permutation(len(en_ids))\n",
    "fr_train, fr_test = fr_padded[idxs][:n], fr_padded[idxs][n:]\n",
    "en_train, en_test = en_padded[idxs][:n], en_padded[idxs][n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   95, 1400,   13,    2,   61,    7,  335,    9, 1280, 1960,\n",
       "          5, 3633, 1334,    8,    2,  445,    5,    2, 3156,  868, 1101,\n",
       "         13, 3633, 1334,    3,    6,   21,   37, 2668,   11,    2,  868,\n",
       "       1101,    7,   19, 1292,   29,    2,  233,    5, 1062,   14,  138,\n",
       "         26, 1599,    7,    2,  505,  443,    9,  255, 1601,  780,    2,\n",
       "         80,    5,    2, 1183,    5, 3633, 1334,    4,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im\n",
      "bericht\n",
      "wird\n",
      "die\n",
      "kommission\n",
      "ersucht\n",
      ",\n",
      "im\n",
      "rahmen\n",
      "des\n",
      "kommenden\n",
      "grünbuchs\n",
      "über\n",
      "den\n",
      "territorialen\n",
      "zusammenhalt\n",
      "eine\n",
      "umfassende\n",
      "definition\n",
      "des\n",
      "territorialen\n",
      "zusammenhalts\n",
      "vorzulegen\n",
      ",\n",
      "und\n",
      "ich\n",
      "kann\n",
      "versichern\n",
      ",\n",
      "dass\n",
      "das\n",
      "für\n",
      "ende\n",
      "september\n",
      "dieses\n",
      "jahres\n",
      "zu\n",
      "erwartende\n",
      "grünbuch\n",
      "zu\n",
      "einem\n",
      "fortschritt\n",
      "im\n",
      "<UNK>\n",
      "gemeinsamen\n",
      "verständnis\n",
      "des\n",
      "konzepts\n",
      "der\n",
      "territorialen\n",
      "kohäsion\n",
      "beitragen\n",
      "wird\n",
      ".\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict((i, word) for word, i in fr_w2id.items())\n",
    "for a in fr_train[0]:\n",
    "    print(reverse_word_index[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to save  \n",
    "look_ups = {'en_w2id':en_w2id,'fr_vocab':fr_vocab,'en_vocab':en_vocab, 'en_embs':en_embs,'fr_embs':fr_embs}\n",
    "dump(look_ups, dpath+'look_upswmt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'fr_train':fr_train,'en_train':en_train,'fr_test':fr_test,'en_test':en_test,}\n",
    "dump(data, dpath+'nmt_datawmt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0266aff821744178bf198185fbbd5cf1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e7384b4c75d437bb1fe8a9693658cb9": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e8fac4707f042aa903fb0d99a10db34": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "10ab8aa7a96e46bfb356fb50841de35b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "1a324d73121342268fb3bdab4e0ddc85": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "26e3d517d1ad4ef4bb1a8667f380e84f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "38965a9c75124e138d24ea4b3f1aaf51": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "39d8f35bd1bf412db5e388165d2e681d": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "4e1524bdbc094d2aaf762062679cad12": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "54e930da987a4d388a10f95ef9d0d29f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "60a3f36c191a48a68b0b5d7043d7ee2c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "7beff7f32b174d6a84b40a07a84298a6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "852f44c6c8ac41a8bf8b52261d56025a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "85514cbcb43b4a53abdd0fb2d1fe9edc": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a27aec095a34b64b559873253a8ceb6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a63b516aeb9441abdff56dd973c2dcb": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "9921ef0dc7714826b9a4dd210002cadd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a1e4974f09e7444d9e724242cd48193c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a53dd4f9998848ca9ac1b303a729dc9a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "abdda4152496465d8e592203c90dc443": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "c48ca8a0c97a4eb1bdd6ace69e114ffd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ce4ee282c7f643649dee064b54a53748": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "def4852d68a14e1ea7576b27b67b6f17": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e04977fa4cf64be59e9e55bcb898f3d7": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e08e2127395a4003a14814472a0af7f4": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ec407f689bac48bab95ac233b45a8976": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eca29ad2f8734cafab240977b6d5b4f1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "edc814ed0e054bafab4417b5fef593e1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eeb590879c164660ae58bf4f4bd18d25": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "fb22a8a5bcda4a97a91076cf88190fd1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ff9739632a7640d8984e8803859c050b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
