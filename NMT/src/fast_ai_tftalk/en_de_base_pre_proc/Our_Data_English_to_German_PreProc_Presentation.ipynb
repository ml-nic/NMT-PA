{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NMT Model\n",
    "\n",
    "This example was taken from the wonderful Cutting Edge Deep Learning for Coders course as taught by Jeremy Howard http://course.fast.ai/part2.html The course is now live and I encourage you to check it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "#import sutils; importlib.reload(sutils)\n",
    "from sutils import *\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import keras.backend as K\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras import initializers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, TensorBoard\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "\n",
    "from recurrentshop import *\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq,SimpleSeq2Seq, Seq2Seq\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we will use **gensim** and **word2vec** to get our embeddings for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/TensorFlowTalks/neural_translation_en_de/'\n",
    "dpath = '/data/TensorFlowTalks/neural_translation_en_de/translate/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(dpath):\n",
    "    os.makedirs(dpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Corpus\n",
    "\n",
    "we will make a limited corpus of English Questions and their partners in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106975\n",
      "Write Tom.\tSchreibt Tom!\n"
     ]
    }
   ],
   "source": [
    "# Split train file in two seperat for each language one:\n",
    "\n",
    "lines = open('/data/wrapper/PA_BA/DataSets/Training/DE_EN_(tatoeba)_train.txt', encoding='UTF-8').read().split('\\n')\n",
    "print(len(lines))\n",
    "print(lines[0])\n",
    "lines_de = []\n",
    "lines_en = []\n",
    "for line in lines:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    lines_en.append(input_text)\n",
    "    lines_de.append(target_text)\n",
    "assert len(lines_de) == len(lines_en)    \n",
    "\n",
    "with(open('/data/wrapper/PA_BA/DataSets/Training/DE_EN_(tatoeba)_train_german_only.txt', 'w', encoding='utf8')) as file:\n",
    "    for idx in range(len(lines_de)):\n",
    "        file.write(lines_de[idx])\n",
    "        if idx != (len(lines_de) - 1):\n",
    "            file.write(\"\\n\")\n",
    "with(open('/data/wrapper/PA_BA/DataSets/Training/DE_EN_(tatoeba)_train_english_only.txt', 'w', encoding='utf8')) as file:\n",
    "    for idx in range(len(lines_en)):\n",
    "        file.write(lines_en[idx])\n",
    "        if idx != (len(lines_en) - 1):\n",
    "            file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/data/wrapper/PA_BA/DataSets/Training/\" + 'DE_EN_(tatoeba)_train_'\n",
    "en_fname = fname + 'english_only.txt'\n",
    "fr_fname = fname + 'german_only.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates the Regex for filtering just for questions\n",
    "#re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "#re_fq = re.compile('^([^?.!]+\\?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this runs our regex search on the full corpus and filters it down\n",
    "#lines = ((re_eq.search(eq), re_fq.search(fq)) \n",
    "#         for eq, fq in zip(open(en_fname, encoding='utf8'), open(fr_fname, encoding='utf8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Write Tom.', 'Schreibt Tom!'),\n",
       " ('What do you like to eat?', 'Was essen Sie gerne?'),\n",
       " ('Tom grabbed Mary to keep her from falling.',\n",
       "  'Tom ergriff Maria, auf dass sie nicht falle.'),\n",
       " ('You really seem to like beer.',\n",
       "  'Du scheinst wirklich ein Bierfreund zu sein.'),\n",
       " ('Tom is feeding the cows.', 'Tom füttert die Kühe.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [(en, de) for en, de in zip(open(en_fname, encoding='utf8').read().split('\\n'), open(fr_fname, encoding='utf8').read().split('\\n'))]\n",
    "questions = lines\n",
    "questions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to put them all in a list so that we can easily access them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = [(e.group(), f.group()) for e,f in lines if e and f]\n",
    "#len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets save this so we can come back to it in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(questions, dpath+'questions.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "loading and unwrapping the raw English/French questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = load(dpath+'questions.pkl')\n",
    "en_qs, fr_qs = zip(*questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the questions into tokens so that we can make sequences for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_mult_space = re.compile(r\"  *\")\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\")\n",
    "re_apos = re.compile(r\"(\\w)'s\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_toks(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['schreibt', 'tom', '!'],\n",
       " ['was', 'essen', 'sie', 'gerne', '?'],\n",
       " ['tom', 'ergriff', 'maria', ',', 'auf', 'dass', 'sie', 'nicht', 'falle', '.'],\n",
       " ['du', 'scheinst', 'wirklich', 'ein', 'bierfreund', 'zu', 'sein', '.']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_qtoks = list(map(simple_toks, fr_qs)); fr_qtoks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['write', 'tom', '.'],\n",
       " ['what', 'do', 'you', 'like', 'to', 'eat', '?'],\n",
       " ['tom', 'grabbed', 'mary', 'to', 'keep', 'her', 'from', 'falling', '.'],\n",
       " ['you', 'really', 'seem', 'to', 'like', 'beer', '.']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_qtoks = list(map(simple_toks, en_qs)); en_qtoks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert tokens to ids so that we can creat lookup tables   \n",
    "\n",
    "we also insert the \"PAD\" token in here\n",
    "\n",
    "this function returns\n",
    "ids - for words\n",
    "vocab -  \n",
    "w2id - is for looking up the \n",
    "voc_cnt - the vocab count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks2ids(sents):\n",
    "    voc_cnt = collections.Counter(t for sent in sents for t in sent)\n",
    "    vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n",
    "    vocab.insert(0, \"<PAD>\")\n",
    "    w2id = {w:i for i,w in enumerate(vocab)}\n",
    "    ids = [[w2id[t] for t in sent] for sent in sents]\n",
    "    return ids, vocab, w2id, voc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13187, 26925)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_ids, fr_vocab, fr_w2id, fr_counts = toks2ids(fr_qtoks)\n",
    "en_ids, en_vocab, en_w2id, en_counts = toks2ids(en_qtoks)\n",
    "len(en_vocab), len(fr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences converted to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 18, 4, 39, 3, 170, 8]\n",
      "['what', 'do', 'you', 'like', 'to', 'eat', '?']\n"
     ]
    }
   ],
   "source": [
    "print(en_ids[1])\n",
    "print(en_qtoks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The look up tables / dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_w2id['do']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "here we are going to make look up tables for words to embeddings\n",
    "\n",
    "The GloVE embeddings used here are 400k words with 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_load_glove(loc):\n",
    "    return (load_array(loc+'.txt'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb'), encoding='latin1'),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb'), encoding='latin1'))\n",
    "def load_glove(loc):\n",
    "    en_wv_word = []\n",
    "    en_wv_idx = {}\n",
    "    en_vecs = []\n",
    "    loc = loc + '.txt'\n",
    "    with(open(loc, encoding='latin1')) as file:\n",
    "        lines = file.readlines()\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        splitted_lines = line.split(' ')\n",
    "        word = splitted_lines[0]\n",
    "        en_wv_word.append(word)\n",
    "        en_wv_idx[word] = index\n",
    "        index += 1\n",
    "        en_vecs.append(splitted_lines[1:len(splitted_lines)])\n",
    "    return (np.asarray(en_vecs), en_wv_word, en_wv_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs, en_wv_word, en_wv_idx = load_glove('/data/TensorFlowTalks/embeddings/glove/6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_w2v = {w: en_vecs[en_wv_idx[w]] for w in en_wv_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_en_vec, dim_en_vec = en_vecs.shape\n",
    "dim_fr_vec = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_en_vec 100\n",
      "n_en_vec 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_en_vec\", dim_en_vec)\n",
    "print(\"n_en_vec\", n_en_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fr_wik = pickle.load(open('/data/TensorFlowTalks/embeddings/french/polyglot-fr.pkl', 'rb'), \n",
    "#                     encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The French embeddings were trained by Jean-Philippe Fauconnier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Word vectors: http://fauconnier.github.io/index.html#wordembeddingmodels\n",
    "- frWac: http://wacky.sslmit.unibo.it/doku.php?id=corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path='/data/TensorFlowTalks/embeddings/german/word_emb_de.bin'\n",
    "\n",
    "fr_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "fr_voc = fr_model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(w2v, targ_vocab, dim_vec):\n",
    "    vocab_size = len(targ_vocab)\n",
    "    emb = np.zeros((vocab_size, dim_vec))\n",
    "\n",
    "    for i, word in enumerate(targ_vocab):\n",
    "        try:\n",
    "            emb[i] = w2v[word]\n",
    "        except KeyError:\n",
    "            # If we can't find the word, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(dim_vec,))\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13187, 100)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embs = create_emb(en_w2v, en_vocab, dim_en_vec); en_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26925, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_embs = create_emb(fr_model, fr_vocab, dim_fr_vec); fr_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lengths = collections.Counter(len(s) for s in en_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_padded = pad_sequences(en_ids, maxlen, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_padded = pad_sequences(fr_ids, maxlen, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106975, 30), (106975, 30), (13187, 100))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_padded.shape, fr_padded.shape, en_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(en_ids)*0.9)\n",
    "idxs = np.random.permutation(len(en_ids))\n",
    "fr_train, fr_test = fr_padded[idxs][:n], fr_padded[idxs][n:]\n",
    "en_train, en_test = en_padded[idxs][:n], en_padded[idxs][n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4, 3758,   34,   28,  356,  112,  108,    1,    4,  152,   35,\n",
       "        103, 2817,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to save  \n",
    "look_ups = {'en_w2id':en_w2id,'fr_vocab':fr_vocab,'en_vocab':en_vocab, 'en_embs':en_embs,'fr_embs':fr_embs}\n",
    "dump(look_ups, dpath+'look_ups.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'fr_train':fr_train,'en_train':en_train,'fr_test':fr_test,'en_test':en_test,}\n",
    "dump(data, dpath+'nmt_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0266aff821744178bf198185fbbd5cf1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e7384b4c75d437bb1fe8a9693658cb9": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e8fac4707f042aa903fb0d99a10db34": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "10ab8aa7a96e46bfb356fb50841de35b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "1a324d73121342268fb3bdab4e0ddc85": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "26e3d517d1ad4ef4bb1a8667f380e84f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "38965a9c75124e138d24ea4b3f1aaf51": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "39d8f35bd1bf412db5e388165d2e681d": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "4e1524bdbc094d2aaf762062679cad12": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "54e930da987a4d388a10f95ef9d0d29f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "60a3f36c191a48a68b0b5d7043d7ee2c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "7beff7f32b174d6a84b40a07a84298a6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "852f44c6c8ac41a8bf8b52261d56025a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "85514cbcb43b4a53abdd0fb2d1fe9edc": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a27aec095a34b64b559873253a8ceb6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a63b516aeb9441abdff56dd973c2dcb": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "9921ef0dc7714826b9a4dd210002cadd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a1e4974f09e7444d9e724242cd48193c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a53dd4f9998848ca9ac1b303a729dc9a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "abdda4152496465d8e592203c90dc443": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "c48ca8a0c97a4eb1bdd6ace69e114ffd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ce4ee282c7f643649dee064b54a53748": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "def4852d68a14e1ea7576b27b67b6f17": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e04977fa4cf64be59e9e55bcb898f3d7": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e08e2127395a4003a14814472a0af7f4": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ec407f689bac48bab95ac233b45a8976": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eca29ad2f8734cafab240977b6d5b4f1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "edc814ed0e054bafab4417b5fef593e1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eeb590879c164660ae58bf4f4bd18d25": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "fb22a8a5bcda4a97a91076cf88190fd1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ff9739632a7640d8984e8803859c050b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
