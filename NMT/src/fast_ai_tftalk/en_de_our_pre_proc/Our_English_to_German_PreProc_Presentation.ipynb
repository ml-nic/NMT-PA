{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from Tokenizer import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file, save_unpreprocessed_targets=False):\n",
    "    \"\"\"\n",
    "    Reads the data from the given file.\n",
    "    The two languages in the file have to be splitted by a tab\n",
    "    :param file: file which should be read from\n",
    "    :return: (input_texts, target_texts)\n",
    "    \"\"\"\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    lines = open(file, encoding='UTF-8').read().split('\\n')\n",
    "    for line in lines:\n",
    "        input_text, target_text = line.split('\\t')\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "    if save_unpreprocessed_targets is True:\n",
    "        val_target_texts_no_preprocessing = target_texts.copy()\n",
    "    assert len(input_texts) == len(target_texts)\n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(train_input_texts, train_target_texts, params, val_input_texts, val_target_texts):\n",
    "        en_tokenizer = Tokenizer(START_TOKEN, END_TOKEN, UNK_TOKEN,\n",
    "                                 num_words=params['MAX_WORDS_EN'])\n",
    "        en_tokenizer.fit_on_texts(train_input_texts)\n",
    "        train_input_texts = en_tokenizer.texts_to_sequences(train_input_texts)\n",
    "        train_input_texts = pad_sequences(train_input_texts, maxlen=params['MAX_SEQ_LEN'],\n",
    "                                               padding='post',\n",
    "                                               truncating='post')\n",
    "        insert_valid_token_at_last_position(train_input_texts, params)\n",
    "        val_input_texts = en_tokenizer.texts_to_sequences(val_input_texts)\n",
    "        val_input_texts = pad_sequences(val_input_texts, maxlen=params['MAX_SEQ_LEN'],\n",
    "                                               padding='post',\n",
    "                                               truncating='post')\n",
    "        insert_valid_token_at_last_position(val_input_texts, params)\n",
    "        en_word_index = en_tokenizer.word_index\n",
    "\n",
    "        de_tokenizer = Tokenizer(START_TOKEN, END_TOKEN, UNK_TOKEN,\n",
    "                                 num_words=params['MAX_WORDS_DE'])\n",
    "        de_tokenizer.fit_on_texts(train_target_texts)\n",
    "        train_target_texts = de_tokenizer.texts_to_sequences(train_target_texts)\n",
    "        train_target_texts = pad_sequences(train_target_texts, maxlen=params['MAX_SEQ_LEN'],\n",
    "                                                padding='post',\n",
    "                                                truncating='post')\n",
    "        insert_valid_token_at_last_position(train_target_texts, params)\n",
    "        val_target_texts = de_tokenizer.texts_to_sequences(val_target_texts)\n",
    "        val_target_texts = pad_sequences(val_target_texts, maxlen=params['MAX_SEQ_LEN'],\n",
    "                                                padding='post',\n",
    "                                                truncating='post')\n",
    "        insert_valid_token_at_last_position(val_target_texts, params)\n",
    "        de_word_index = de_tokenizer.word_index\n",
    "\n",
    "        embeddings_index = {}\n",
    "        filename = \"/data/wrapper/PA_BA/DataSets/glove.6B.200d.txt\"\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            for line in f.readlines():\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        num_train_words = params['MAX_WORDS_EN'] + 3\n",
    "        en_embedding_matrix = np.zeros((num_train_words, params['EMBEDDING_DIM']))\n",
    "        for word, i in en_word_index.items():\n",
    "            if i >= params['MAX_WORDS_EN'] + 3:\n",
    "                continue\n",
    "            embedding_vector = None\n",
    "            if word == START_TOKEN:\n",
    "                embedding_vector = START_TOKEN_VECTOR\n",
    "            elif word == END_TOKEN:\n",
    "                embedding_vector = END_TOKEN_VECTOR\n",
    "            elif word == UNK_TOKEN:\n",
    "                embedding_vector = UNK_TOKEN_VECTOR\n",
    "            else:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is None:\n",
    "                embedding_vector = UNK_TOKEN_VECTOR\n",
    "            en_embedding_matrix[i] = embedding_vector\n",
    "        return train_input_texts, train_target_texts, en_word_index, de_word_index, en_embedding_matrix, val_input_texts, val_target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_valid_token_at_last_position(texts, params):\n",
    "        for sent in texts:\n",
    "            if not (sent[params['MAX_SEQ_LEN'] - 1] == 0 or sent[params['MAX_SEQ_LEN'] - 1] == 2):\n",
    "                sent[params['MAX_SEQ_LEN'] - 1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "#params['batch_size'] = 64\n",
    "#params['val_batch_size'] = 256\n",
    "#params['epochs'] = 20\n",
    "#params['latent_dim'] = 1000\n",
    "params['MAX_SEQ_LEN'] = 100\n",
    "params['EMBEDDING_DIM'] = 200\n",
    "params['MAX_WORDS_DE'] = 40000\n",
    "params['MAX_WORDS_EN'] = 40000\n",
    "#params['P_DENSE_DROPOUT'] = 0.2\n",
    "#params['VALIDATION_FREQ'] = 1\n",
    "\n",
    "identifier = \"my_pre_proc\"\n",
    "\n",
    "BASE_DATA_DIR = \"/data/TensorFlowTalks/neural_translation_my_pre_proc/\"\n",
    "BASIC_PERSISTENCE_DIR = \"/data/TensorFlowTalks/neural_translation_my_pre_proc/translate/\"\n",
    "TRAIN_DATA_FILE = os.path.join(BASE_DATA_DIR, 'DE_EN_(tatoeba)_train.txt')\n",
    "VAL_DATA_FILE = os.path.join(BASE_DATA_DIR, 'DE_EN_(tatoeba)_validation.txt')\n",
    "\n",
    "START_TOKEN = \"_GO\"\n",
    "END_TOKEN = \"_EOS\"\n",
    "UNK_TOKEN = \"_UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN_VECTOR = np.random.rand(params['EMBEDDING_DIM'])\n",
    "END_TOKEN_VECTOR = np.random.rand(params['EMBEDDING_DIM'])\n",
    "UNK_TOKEN_VECTOR = np.random.rand(params['EMBEDDING_DIM'])\n",
    "\n",
    "train_input_texts, train_target_texts = split_data(TRAIN_DATA_FILE)\n",
    "val_input_texts, val_target_texts = split_data(VAL_DATA_FILE)\n",
    "num_train_samples = len(train_input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "train_input_texts, train_target_texts, en_word_index, de_word_index, en_embedding_matrix, val_input_texts, val_target_texts = create_vocab(train_input_texts, train_target_texts, params, val_input_texts, val_target_texts)\n",
    "np.save(BASIC_PERSISTENCE_DIR + '/train_target_texts.npy', train_target_texts)\n",
    "np.save(BASIC_PERSISTENCE_DIR + '/train_input_texts.npy', train_input_texts)\n",
    "np.save(BASIC_PERSISTENCE_DIR + '/val_target_texts.npy', val_target_texts)\n",
    "np.save(BASIC_PERSISTENCE_DIR + '/val_input_texts.npy', val_input_texts)\n",
    "np.save(BASIC_PERSISTENCE_DIR + '/en_word_index.npy', en_word_index)\n",
    "np.save(BASIC_PERSISTENCE_DIR + '/de_word_index.npy', de_word_index)\n",
    "np.save(BASIC_PERSISTENCE_DIR + '/en_embedding_matrix.npy', en_embedding_matrix)\n",
    "\n",
    "num_train_samples = len(train_input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0266aff821744178bf198185fbbd5cf1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e7384b4c75d437bb1fe8a9693658cb9": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "0e8fac4707f042aa903fb0d99a10db34": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "10ab8aa7a96e46bfb356fb50841de35b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "1a324d73121342268fb3bdab4e0ddc85": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "26e3d517d1ad4ef4bb1a8667f380e84f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "38965a9c75124e138d24ea4b3f1aaf51": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "39d8f35bd1bf412db5e388165d2e681d": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "4e1524bdbc094d2aaf762062679cad12": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "54e930da987a4d388a10f95ef9d0d29f": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "60a3f36c191a48a68b0b5d7043d7ee2c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "7beff7f32b174d6a84b40a07a84298a6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "852f44c6c8ac41a8bf8b52261d56025a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "85514cbcb43b4a53abdd0fb2d1fe9edc": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a27aec095a34b64b559873253a8ceb6": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "8a63b516aeb9441abdff56dd973c2dcb": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "9921ef0dc7714826b9a4dd210002cadd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a1e4974f09e7444d9e724242cd48193c": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "a53dd4f9998848ca9ac1b303a729dc9a": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "abdda4152496465d8e592203c90dc443": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "c48ca8a0c97a4eb1bdd6ace69e114ffd": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ce4ee282c7f643649dee064b54a53748": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "def4852d68a14e1ea7576b27b67b6f17": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e04977fa4cf64be59e9e55bcb898f3d7": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "e08e2127395a4003a14814472a0af7f4": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ec407f689bac48bab95ac233b45a8976": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eca29ad2f8734cafab240977b6d5b4f1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "edc814ed0e054bafab4417b5fef593e1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "eeb590879c164660ae58bf4f4bd18d25": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "fb22a8a5bcda4a97a91076cf88190fd1": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    },
    "ff9739632a7640d8984e8803859c050b": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
